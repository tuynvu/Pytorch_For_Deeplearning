################### Mục lục Pytorch Quy Trình Làm Việc #################################
1. Tổng quan
2. Dữ liệu (data)
3. Phân chia dữ liệu để training và testing
4. Xây dựng model
5. Điều cần thiết khi xậy dựng mô hình Pytorch
6. Kiểm tra nội dung của model Pytorch
7. Tạo dự đoán với torch.inference_mode()
8. Training Model
9. Tạo Hàm mất mát (loss) và tối ưu (optimize) trong pytorch
10. Tạo vòng lặp tối ưu hóa trong Pytorch

########################################### END ##########################################

_________________________________________________________________________________________________________________________
1. Tổng quan

 - Bản chất của ML và DL là dử dụng các bộ dữ liệu có sẵn trong thời đại dữ liệu rất nhiều hiện nay, và từ đó bạn dử dụng
 các dữ liệu có ích cho mô hình của mình, và dụng thụât toán tạo nên các bộ số phù hợp với các tham số
 - Trong đố số lượn tham số rất quan trọng trong tinh chỉnh
 - Từ đó ta có thể sử dụng các dữ liệu thực tế để tạp ta tính chấ trong tương lai với các xác xuất chính xác tùy theo model

 - Những thứ mà tôi nói trong bài viết này:
    + Lấy dữ liệu : rất nhiều dữ liệu
    + Xậy dựng model: Ta sẽ tạo dụng mô hình cho học các dữ liệu: ta sẽ chọn loss function, optimizer, vòng lập trainning
    + Chạy vòng lặp training trên các dữ liệu thu thập: ta đã có data và model nên ta sẽ tìm các hệ số phù hợp với nó
    + Tạo dự đoán với các dữ liệu khác và đánh giá
    + Lưu và Loading model để sử dùng lại
    + Cách đưa cho người khác sử dụng

_________________________________________________________________________________________________________________________
2. Dữ liệu (data)

- Data là những thứ trong thực tế: có thể là ảnh, video, các bảng số, .....
- Khi mà bạn có những dữ liệu đó mà những dữ liệu thực tế đó máy chúng ta không thể hiểu mà máy chỉ có thể hiểu những con
số, vì thế bạn cần cách chuyển đổi nó sang số
- Bạn cần biết cách biểu diễn các dữ liệu đó vào máy để nó có thể hiểu rằng đó là của dữ liệu nào: kiểu cấu trúc nào
phù hợp với dữ liệu của bạn có thể là vector, matrix, siêu mặt phẳng

- Để thực hiện việc chơi ML:
    + Bước 1: bạn cần chuẩn bị dữ liệu tốt, dữ liệu mà biểu dĩễn, đại diện cho việc mà chúng ta cần làm
    + Bước 2: bạn tìm kiếm 1 model hay tự xây dựng 1 model với các tham số phù hợp với đầu ra

- Khi bạn thực hiện được nó thì bạn có thể tạo ra 1 mô hình ML
- Khi mà bạn chưa có dữ liệu:
- Ta có thể tạo ra đường thẳng theo các tham số ngẫu nhiên đó là mô hình linear regression

EX:
    >>> weight = 0.4
    >>> bias = 0.6

    >>> x = torch.arange(1, 4, 1).unsqueeze(1)
    >>> y = x * weight + bias

    >>> x, y
    (tensor([[1],
             [2],
             [3]]),
     tensor([[1.0000],
             [1.4000],
             [1.8000]]))

- Các bạn có thể đố ở mô hình trên có các tham số: weight, bias
- Đó là các param ngẫu nhiên
- Để nó có nghĩa là ta cần điểu chỉnh các tham số đó theo bộ dữ liệu chúng ta mong muốn

_________________________________________________________________________________________________________________________
3. Phân chia dữ liệu để training và testing

- Việc quan trọng trong việc hoàn thiện model đó là ta chia bộ dữ liệu mà chúng ta có sẵn thành 2 hoặc 3 phần:
    + train data: bộ dữ liệu để trainning: luôn có
    + validation data: bộ dữ liệu thẩm định: có thể có hoặc không
    + test data: tạo bộ dữ liệu sau khi train model để có thể xét tính khả thi của nó

EX:
    >>> split_train = int(0.8 * len(x))
    >>> train_x, test_x, train_y , test_y = x[:split_train], x[split_train:], y[:split_train], y[split_train:]
    >>> len(train_x), len(train_y), len(test_x), len(test_x)
    (40, 40, 10, 10)

+ Ta sẽ hiển thị x,y lên đồ thị
    >>> import matplotlib.pyplot as plt

    >>> def plot_predict(data_x=train_x, data_y=train_y, data_test_x=test_x, data_test_y=test_y, predictions=None):

            plt.scatter(data_x, data_y, s=4, label="train")
            plt.scatter(data_test_x, data_test_y, s=20, label="test")
            if predictions is not None:
                plt.scatter(data_test_x, predictions, s=4, label="predictions")

            plt.legend()
            plt.show()

        plot_predict(predictions=predictions)

- Việc không kém quan trọng trong ML đó là việc vizualize dữ liệu, ta cần trực quan hóa dữ liệu để có thể điều chỉnh tham
số cho phù hợp nhất có thể

_________________________________________________________________________________________________________________________
4. Xây dựng model

- Bây giờ chúng ta xây dựng 1 model sử dụng các chấm blue để dự đoán các chấm green
- Bây giờ ta sẽ tái tạo mô hình linear regression với pytorch thuần túy
- Model Linear Regression:
    class LinearRegression(torch.nn.Module): # module nn.Module là nó chứa hầu hết các thứ trong torch, các khổi
        def __init__(self):
            super(LinearRegression, self).__init__() # chạy hàm khởi tạo bên thằng cha
            self.weight = torch.nn.Parameter(torch.randn(1, # size của rand
                                                         dtype=torch.float32), # loại cho torch.Tensor
                                             requires_grad=True) # Có thể cập nhât giá trị theo độ dốc (Đạo Hàm)
            self.bias = torch.nn.Parameter(torch.randn(1,
                                                       dtype=torch.float32),
                                           requires_grad=True)


        def forward(self, x: torch.Tensor) -> torch.Tensor:
            return self.weight * x + self.bias # tính toán và trả về dự đoán

- Có nhiều điều ở trên tôi sẽ giải thích rõ hơn phần sau

_________________________________________________________________________________________________________________________
5. Điều cần thiết khi xậy dựng mô hình Pytorch

- Ở Pytorch đã cho ta 4 Module cần thiết, có thể tạo ra hầu hết mọi loại mạng mà ta cần thiết
- Các Module đó là:
    + torch.nn: Module mà ta đã sử dụng ở trên
        -> Chứa tất cả các khối xây dựng cho đồ thị tính toán (về cơ bản là một chuỗi các phép tính được thực hiện theo
        một cách cụ thể).
    + torch.nn.Parameter:
        -> Lưu trữ các tensor có thể được sử dụng với nn.Module. Nếu require_grad=True gradient (được sử dụng để cập
        nhật các tham số mô hình thông qua việc giảm độ dốc) được tính toán tự động, thì điều này thường được gọi là
        "autograd".
    + torch.nn.Module:
        -> Lớp cơ sở cho tất cả các mô-đun mạng nơ-ron, tất cả các khối xây dựng cho mạng nơ-ron đều là các lớp con.
        Nếu bạn đang xây dựng mạng nơ-ron trong PyTorch, mô hình của bạn nên phân lớp nn.Module. Yêu cầu triển khai
        phương thức Forward().
    + torch.optim :
        -> Chứa các thuật toán tối ưu hóa khác nhau (những thuật toán này cho các tham số mô hình được lưu trữ trong
        nn.Parameter cách thay đổi tốt nhất để cải thiện độ dốc giảm dần và từ đó giảm tổn thất).

    + def forward(): luôn cần ghi đè cách tính toán của model ta tự tạo
        -> Tất cả các lớp con nn.Module đều yêu cầu phương thức Forward(), phương thức này xác định tính toán sẽ diễn ra
         trên dữ liệu được truyền đến nn.Module cụ thể (ví dụ: công thức hồi quy tuyến tính ở trên).
    + torch.autograd():
        -> chế độ tự động tính độ dốc (đạo hàm) của nn.Module
- Nếu mà các bạn thấy bên trên khó hiểu thì bạn có thể coi hầu hết mọi thứ trong mạng nơ ron pytorch đến từ torch.nn
    + nn.Module: chứa các khối xây dựng lớn hơn (Layer)
    + nn.Parameter: chứa các tham số nhỏ hơn như weight và bias, và đặt cạnh nhau ta tao thành nn.Module
    + forward(): nói các khối lớn hơn (nn.Module) tạo tính toán trên input (input đủ thông tin)
    + torch.optim: chứa các phương thức tối ưu hóa tính toán giảm dần của độ dốc (đạo hàm), cải thiện tham số cho model

- Note: Khi mà tạo 1 model NN từ lớp con của nn.Module thì ta luôn luôn phải định nghĩa phương thức "forward()"

_________________________________________________________________________________________________________________________
6. Kiểm tra nội dung của model Pytorch

- Khi mà bạn tạo 1 mô hình nào đó nhưng bạn không biết nó có những gì thì ta không thể sửa và nâng cấp nó thêm được
- Nên bây giờ ta vào cách xem nội dung của model đó chứa gì
- Bạn có 1 số câu hỏi cho model ta viết hay của ai đó: weight là gì, ....

- Xem weight ta dùng phương thức:
    -> .parameter()
EX:
    >>> torch.manual_seed(42)
    >>> linear = LinearRegression()
    >>> print(list(linear.parameters()))
    [Parameter containing:
    tensor([0.3367], requires_grad=True), Parameter containing:
    tensor([0.1288], requires_grad=True)]

- Chúng ta cũng có thể lấy trạng thái (mô hình chứa gì) của mô hình bằng cách sử dụng
    -> .state_dict()
    >>> linear.state_dict()
    OrderedDict([('weight', tensor([0.3367])), ('bias', tensor([0.1288]))])

- ở trên cá bạn thấy các số weight và bias điều đó là do ta dùng hàm torch.rand
- Và bản chất của nó là ta tạo ngẫu nhiên các tham số đó và cập nhật các tham số theo bộ dữ liệu mà ta tạo ra
- Và bây giờ model của chúng ta dự đoán kém bới các hệ số chưa được trainning

_________________________________________________________________________________________________________________________
7. Tạo dự đoán với torch.inference_mode()
- Để kiểm tra điều này, chúng ta có thể chuyển cho nó dữ liệu thử nghiệm X_test để xem nó dự đoán y_test chặt chẽ đến
mức nào.
- Khi chúng ta truyền dữ liệu đến mô hình của mình, nó sẽ đi qua phương thức Forward() của mô hình và tạo ra kết quả
bằng cách sử dụng tính toán mà chúng ta đã xác định.
- Bây giờ chúng ta đưa ra một số dự đọán:
EX:
    >>> with torch.inference_mode():
            y_preds = linear(test_x)

- Bạn có thể nhận thấy rằng chúng tôi đã sử dụng torch.inference_mode() làm trình quản lý bối cảnh (đó là chức năng của
torch.inference_mode():) để đưa ra dự đoán.
- Đúng như tên gọi, torch.inference_mode() được sử dụng khi sử dụng mô hình để suy luận (đưa ra dự đoán).
- torch.inference_mode() tắt rất nhiều thứ (như theo dõi độ dốc, cần thiết cho việc đào tạo nhưng không cần cho suy
luận) để thực hiện chuyển tiếp (dữ liệu đi qua phương thức Forward()) nhanh hơn.

- Lưu ý: Trong mã PyTorch cũ hơn, bạn cũng có thể thấy torch.no_grad() được sử dụng để suy luận. Trong khi
torch.inference_mode() và torch.no_grad() thực hiện những việc tương tự, thì torch.inference_mode() mới hơn, có khả năng
nhanh hơn và được ưa thích hơn. Xem Tweet này từ PyTorch để biết thêm.
    -> torch.inference_mode() <=> torch.no_grad()
    -> bởi vì nó tắt hết chế độ theo dõi độ dốc (đạo hàm)
EX:
    >>> print(f"Number of testing samples: {len(test_x)}")
    >>> print(f"Number of predictions made: {len(y_preds)}")
    >>> print(f"Predicted values:\n{y_preds}")
    Number of testing samples: 10
    Number of predictions made: 10
    Predicted values:
    tensor([[0.3982],
            [0.4049],
            [0.4116],
            [0.4184],
            [0.4251],
            [0.4318],
            [0.4386],
            [0.4453],
            [0.4520],
            [0.4588]])

- Ta sẽ vẽ thử để quan sát dự đoán đó
    >>>plot_predict(predictions=y_preds)
    >>> test_y - y_preds
    tensor([[0.5218],
            [0.5231],
            [0.5244],
            [0.5256],
            [0.5269],
            [0.5282],
            [0.5294],
            [0.5307],
            [0.5320],
            [0.5332]])

- Ôi! Những dự đoán đó có vẻ khá tệ...
- Tuy nhiên, điều này có ý nghĩa khi bạn nhớ rằng mô hình của chúng ta chỉ sử dụng các giá trị tham số ngẫu nhiên để
đưa ra dự đoán.
- Nó thậm chí còn không nhìn vào các chấm Blue để cố gắng dự đoán các chấm Green lá cây.
- Đã đến lúc thay đổi điều đó.

_________________________________________________________________________________________________________________________
8. Training Model

- Hiện tại, mô hình của chúng tôi đang đưa ra dự đoán bằng cách sử dụng các tham số ngẫu nhiên để tính toán, về cơ bản
là đoán (ngẫu nhiên).
- Để khắc phục điều đó, chúng tôi có thể cập nhật các tham số bên trong của nó (tôi cũng gọi các tham số là mẫu), trọng
số và giá trị độ lệch mà chúng tôi đặt ngẫu nhiên bằng cách sử dụng nn.Parameter() và torch.randn() để thể hiện dữ liệu
tốt hơn.
- Chúng ta có thể mã hóa cứng điều này (vì chúng ta biết các giá trị mặc định là trọng số = 0,7 và độ lệch = 0,3) nhưng
điều đó có gì thú vị?
- Phần lớn thời gian bạn sẽ không biết các thông số lý tưởng cho một mô hình là gì.
- Thay vào đó, sẽ thú vị hơn nhiều khi viết mã để xem liệu mô hình có thể thử và tự tìm ra chúng hay không.

_________________________________________________________________________________________________________________________
9. Tạo Hàm mất mát (loss) và tối ưu (optimize) trong pytorch

- Để cập nhật mô hình nghĩa là thay đổi weight và bias ta cần có nhứng phương thức trên
- Trước khi tạp ta cần biết những thứ đó là gì?
- Loss function:
    + Pros: tính toán chênh lêch của đầu ra của mô hình (y_preds) và dữ liệu thực tế (test_y)
    + Nó nằm ở: torch.nn
    + Có 2 loại phổ biến:
        -> MAE: mean absolute error: tihs toán trung bình của trị tuyệt đối các đầu ra dành cho regression
            => torch.nn.L1Loss()
        -> Cho phân loại nhị phân
            => torch.nn.BCELoss()
- Optimize function:
    + Pros: cách mà chúng ta cập nhật tham số weight và bias
    + Nó nằm ở: torch.optim
    + Có 2 loại cơ bản:
        -> SGD: stochastic gradient descent
            => torch.optim.SGD(params, lr)
        -> Adam oprimizer
            => torch.optim.Adam(params, lr)

- Bây giờ ta tạo các loss và optim để cải thiện model trên
- Tùy vào các model mà bạn có thể dùng loss và optim nào
EX:
    >>> loss = torch.nn.L1Loss()
    >>> optimizer = torch.optim.SGD(params=linear.parameters(), lr=0.01)
    L1Loss()
    SGD (
    Parameter Group 0
        dampening: 0
        differentiable: False
        foreach: None
        lr: 0.01
        maximize: False
        momentum: 0
        nesterov: False
        weight_decay: 0
    )

_________________________________________________________________________________________________________________________
10. Tạo vòng lặp tối ưu hóa trong Pytorch



