############################# Mục Lục ##############################################################################
1. Cách cài pytorch vào môi trường pip or anaconda
2. Thêm modul Pytorch vào chương trình
3. Tensor
4. Tạo tensor và cơ bản
5. Tensor random
6. Tensor Zeros và Ones
7. Tạo tensor trong range và Tensor tương tự (tensor like)
8. Kiểu dữ liệu torch
9. Lấy thông tin từ Tensor
10. Thao tác cơ bản với tensor - Toán tử với tensor
11. Một số lỗi chung phổ biến trong DL:  lỗi shape
12. Lớp torch.nn.Linear()
13. Tìm min, max, mean, std, .....
14. Vị trí min/max
15. Thay đổi kiểu dữ liệu trong tensor
16. Reshaping, stacking, squeezing, unsqueezing
17. Chỉ số, lấy dữ liệu tử tensor
18. Pytorch Tensor && Numpy Array
19. Cố gắng tái sản xuất ( lấy ngẫu nhiên ra khỏi ngẫu nhiên )
20. Chạy tensor trên GPU ( thức hiện tính toán nhanh hơn )
21. Bài Tập
22. Học Thêm
####################################################################################################################

_______________________________________________________________________
1. Cách cài pytorch vào môi trường pip or anaconda

- Cài bằng pip:

    update pip: pip install --upgrade pip
    install package: pip install torch torchvision torchaudio

- Cài bằng conda:

    update conda: conda update conda
    install package:    
        + conda create --name <env_name> python=3.11
        + conda activate <env_name>
        + conda install pytorch torchvision torchaudio -c pytorch
    update package: conda update --all , conda update namepackage

_______________________________________________________________________
2. Thêm modul Pytorch vào chương trình

- Cách hiểu tương tự như include, import trong c++, java
+ Syntax:
    >>> import torch
+ Kiểm tra version: 
    >>> torch.__version__
    '1.13.1+cu117'

=> cài đặt thành công

_______________________________________________________________________
3. Tensor

- Tạo khối trong ML và DL
- Tạo nên các khối số đại diện cho dữ liệu
- Ta có thể nói về ảnh
    + ta tạo nên ảnh với chiều [3, 600, 620] như trong phần code
    điều đó tương đương [màu, chiều rộng, chiều cao]
    màu có 3 chiều <=> (R, G, B)
- Tensor bạn có thể hiểu như cách dùng trong numpy

_______________________________________________________________________
4. Tạo tensor và cơ bản

- Bài tập đầu tiên của bạn là đọc tài liệu: https://pytorch.org/docs/stable/tensors.html
trong 10'

- Kiểu dữ liệu: có 3 cách dùng:
    + dùng cho máy 
    + CPU : torch.FloatTensor (float32)
    + GPU : torch.cuda.FloatTensor (float32)
- Sytax:  
    # tạo tensor
    >>> a =  torch.tensor([[1, 2, 3]])
    tensor([[1, 2, 3]])
    # điều này có nghĩa là tạo 1 tensor, type là tensor

    # kiểm tra số chiều
    >>> a.ndim
    2

    # lấy số từ tensor
    >>> a[0, 0].item()
    1
    >>> a[0, 0]
    tensor(1)

    # kiểm tra dạng trong tensor
    # Xác định bằng cách: từng ngoặc 1
    >>> a.shape
    torch.Size([1, 3])

Ví Dụ Biểu Diễn Thực Tế:
- Dữ liệu chúng tôi vừa tạo có thể là số liệu bán hàng của một cửa 
hàng bít tết và bơ hạnh nhân .
    >>> TENSOR = torch.tensor([[[1, 2, 3],
                                [3, 6, 9],
                                [2, 4, 5]]])
    # Hàng đầu tiên ngày 
    # hàng 2 là bít tết
    # hàng 3 hạnh nhân

- Các loại tensor: 
    + 1 dim : vector
    + 2 dim : matrix
    + n dim: tensor đa chiều

_______________________________________________________________________
5. Tensor random

- Thay vì các bạn tự chuẩn bị dữ liệu ta có thể random dữ liệu
- Cách này cũng mang lại hiệu quả nhất định cho model

Syntax:
    >>> rd = torch.rand(size=(3, 4))
    >>> rd
    tensor([[0.7241, 0.8427, 0.8053, 0.9273],
            [0.5204, 0.0860, 0.5999, 0.7973],
            [0.8827, 0.3033, 0.6519, 0.1351]])
    # Trong ví dụ trên thì khi gọi rand với size= (3 , 4), ta được ra một matrix
    # với các phần tử (0, 1)
    # Bạn có thể làm nó với ảnh [224, 224, 3] ([height, width, color_channels])

_______________________________________________________________________
6. Tensor Zeros và Ones

- K cần tạo bằng tay ta có thể dùng phương thức có sẵn trong torch
Syntax:
    >>> zr = torch.zeros(size=(3, 4))
    >>> zr
    tensor([[0., 0., 0., 0.],
            [0., 0., 0., 0.],
            [0., 0., 0., 0.]])
    >>> on = torch.ones(size=(3, 4))
    >>> on
    tensor([[1., 1., 1., 1.],
            [1., 1., 1., 1.],
            [1., 1., 1., 1.]])

    # Mặc định kiểu dữ liệu là float32
    >>> on.dtype
    torch.float32

    # Chúng ta có thể  thay đổi nó
    >>> zr = torch.zeros(size=(3, 4), dtype=torch.int32)
    >>> zr
    tensor([[0, 0, 0, 0],
            [0, 0, 0, 0],
            [0, 0, 0, 0]], dtype=torch.int32)

_______________________________________________________________________
7. Tạo tensor trong range và Tensor tương tự (tensor like)

- tensor like method: full_like, zeros_like, ones_like, rand_like, randint_like
Syntax:
    >>> ar = torch.arange(1, 6, 2) # (start, end, step)
    >>> ar
    tensor([1, 3, 5])

    >>> tensor_like_ar = torch.full_like(ar, 1)
    >>> tensor_like_ar
    tensor([1, 1, 1])
    # full_like: nó chỉ giống hình dạng của ar và điền giá trị khác value=1

    >>> ones_ar = torch.ones_like(ar)
    >>> ones_ar
    tensor([1, 1, 1])

_______________________________________________________________________
8. Kiểu dữ liệu torch

- Kiểu dữ liệu: torch.int(8, 16, 32, 64)
                torch.float(8, 16, 32, 64)
                torch.boolean
- Trong CPU: torch.FloatTensor, torch.DoubleTensor
             torch.ShortTensor,torch.IntTensor, torch.LongTensor
             torch.BoolTensor
- Trong GPU tương tự CPU: thay "torch." bằng "torch.cuda."

Ví dụ:
    # Default datatype for tensors is float32
    >>> float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                                        dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed
                                        device=None, # defaults to None, which uses the default tensor type
                                        requires_grad=False) # if True, operations performed on the tensor are recorded 

    >>> float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device
    (torch.Size([3]), torch.float32, device(type='cpu'))
        
    >>> float_16_tensor = torch.tensor([3.0, 6.0, 9.0],
                                       dtype=torch.float16) # torch.half would also work

    >>> float_16_tensor.dtype
    torch.float16

_______________________________________________________________________
9. Lấy thông tin từ Tensor

- 3 thuộc tính phổ biến trong tensor mà bạn thấy trước đó:
    + shape: Dạng của tensor là gì?
    + device: Thiết bị chạy của tensor là gì?
    + dtype: Loại dữ liệu của các phần tử được lưu trong tensor là gì?
- Tạo ra random tensor thì:
    + tensor.rand(size): mỗi phần tử trong khoảng từ 0 -> 1
    EX:
    >>> rd_tensor = torch.rand(size=(3, 4))
    tensor([[0.2159, 0.3589, 0.9351, 0.7204],
            [0.4696, 0.4310, 0.5163, 0.5026],
            [0.2548, 0.7579, 0.4583, 0.5948]])
    >>> rd_tensor.shape
    torch.Size([3, 4])
    >>> rd_tensor.device
    device(type='cpu')
    >>> rd_tensor.dtype
    torch.float32
- Khi ta chạy các chường trình pytorch, lỗi có thể rất liên quan đến 3 thuộc tính ở trên
và để fix lỗi thì các bạn có thể in ra các thuộc tính để sửa lỗi đơn giản hơn
    + Bạn có thể đặt ra các câu hỏi:
    -> tensor của mình hình dạng gì
    -> kiểu dữ liệu của nó là gì: float32, int64 hay bool
    -> nó đang chạy trên thiết bị gì ở đâu
    Mong bạn thấy điều này thú vị

_______________________________________________________________________
10. Thao tác cơ bản với tensor - Toán tử với tensor

- Trong học sâu, dữ liệu có thể là (ảnh, text, video, audio, ......) và các dữ liệu đó được biểu
diễn dưới hình dạng của 1 tensor cho 1 loại dữ liệu ở trên

- Một mô hình được tạo lên là sự thao tác trên các đối tượng tensor đó và các thuật toán đều liên
quan dến tensor đó

- Các tensor đại diện cho các dữ liệu đầu vào vì nó biểu diễn dữ liệu, nó đại diện cho dữ liệu dầu vào
dữ liệu train, test

- Ta có thể coi tensor là các đồ vật thực tế được chuyển thể biểu diễn cho máy tính hiểu đó là Tensor
trong pytorch

- Các thao tác cơ bản:
    + cộng: +
    + trừ: -
    + nhân: *
    + chia: /
    + nhân ma trận: @ (trong numpy)
  Còn các thao tác khác nữa, nhưng đây là các thao tác cơ bản xây dựng lên mạng NN (neural network)

- Như các bạn đã biết mạng nơ ron được cấu tạo từ các ma trận trọng số W, và bạn xếp chồng các matrix
đó lên nhau và bạn đã tạo nên một mạng nơ ron phức tạp từ các matrix đó
    -> tip: bạn có thể hình dung như bạn đang xếp lego vậy :)))

- Các thao tác cơ bản ở trên bạn có thể hiểu cách dùng của nó giống như trong numpy

a, Toán tử cơ bản: + - * /
* Thao tác 1 tensor với 1 số
-> Từng phần tử 1 trong tensor sẽ thao tác toán tử với các số đó
EX:
    >>> tensor1 = torch.rand((1, 3))
    tensor([[0.9866, 0.0876, 0.4429]])

    >>> tensor1 + 1
    tensor([[1.9866, 1.0876, 1.4429]])
    >>> tensor1 - 1
    tensor([[-0.0134, -0.9124, -0.5571]])
    >>> tensor1 * 2
    tensor([[1.9733, 0.1752, 0.8857]])
    >>> tensor1 / 2
    tensor([[0.4933, 0.0438, 0.2214]])

* Thao tác 1 tensor với 1 tensor khác
-> Từng phần tử đôi một thao tác toán tử với nhau
EX:
    >>> tensor1 = torch.rand((1, 3))
    >>> tensor2 = torch.rand((1, 3))
    >>> tensor1, tensor2
    (tensor([[0.8398, 0.8819, 0.3660]]), tensor([[0.5670, 0.9662, 0.9384]]))

    >>> tensor1 + tensor2
    tensor([[1.4069, 1.8481, 1.3044]])
    >>> tensor1 - tensor2
    tensor([[ 0.2728, -0.0842, -0.5724]])
    >>> tensor1 * tensor2
    tensor([[0.4762, 0.8521, 0.3434]])
    >>> tensor1 / tensor2
    tensor([[1.4812, 0.9128, 0.3900]]) # chú ý phần tử 0

* các bạn cũng không nhất thiết dùng các toán tử + - * / mà nó thể dùng các phương thức
khi các bạn quen với java
    + torch.add()
    + torch.sub()
    + torch.multiply()
    + torch.divide()

b, Nhân Matrix Tensor (toán tử @: trong numpy)

- Nó rất quan trọng cho các mô hình học sâu, phần lớn liên quan đến matrix tensor
- Toán tử : @ (giống numpy)
- Cách nhân matrix bạn đã học trong đại số tuyến tính (linear algebra)
    * Mình sẽ nhăc lại chút quy tăc nhân: cần thiếu nhất là shape(a, b) @ shape(b, c)
    * số dòng của matrix trước bằng số cột matrix sau
    * Và thứ tự rất quan trọng trong nhân matrix
EX: shape
    (3, 2) @ (3, 2) won't work
    (2, 3) @ (3, 2) will work
    (3, 2) @ (2, 3) will work
    (2, 3) @ (3, 2) -> (2, 2)
    (3, 2) @ (2, 3) -> (3, 3)

EX:
    >>> tensor = torch.tensor([1, 2, 3])
    >>> tensor.shape
    torch.Size([3])
    >>> tensor * tensor
    tensor([1, 4, 9])
    >>> torch.matmul(tensor, tensor)
    tensor(14)
    >>> tensor @ tensor
    tensor(14)

- Ở đây ta cũng có thể không dùng toán tử @ mà thay vào đó torch.matmul(), torch.mm()
- Ta có thể xây dựng thủ công nhân matrix:
EX:
# Tự xây dựng mulmatrix
    >>> %%time
    >>> value = 0
    >>> for i in range(len(tensor)):
            value += tensor[i] * tensor[i]
    >>> value
    tensor(14)
    CPU times: user 2.37 ms, sys: 0 ns, total: 2.37 ms
    Wall time: 6.18 ms

# Modul sẵn
    >>> %%time
    >>> torch.matmul(tensor, tensor)
    tensor(14)
    CPU times: user 85 µs, sys: 0 ns, total: 85 µs
    Wall time: 86.8 µs

-> Từ 2 ví dụ trên bạn thấy rằng hàm tạo sẵn của torch có tốc độ chạy rất nhanh so với tự tạo
-> Bạn nên áp dụng torch.matmul() để cải thiện tốc độ của mô hình

_______________________________________________________________________
11. Một số lỗi chung phổ biến trong DL:  lỗi shape

- Phần lớn trong DL là phép nhân và thực hiện các toán tử trên ma trận vì thế mà hình dạng của ma trận cần được kiểm
soát nghiêm ngặt và đúng đinh dạng cho phép nhân. Vì thế lỗi phổ biến trong DL là bị sai về hình dạng
- Cần lưu ý khi ta thực hiện toán tử với matrix thì ta cần chú ý đến dạng của tensor mà chúng ta đang thao tác là gì
- Nên ta có thể biết dạng của nó trước khi thực hiện thao tác với nó
    + Ta kiếm tra bằng thuộc tính shape ta đã học ở trên

EX:
    >>> tensor_A = torch.tensor([[1, 2],
                                [3, 4],
                                [5, 6]], dtype=torch.float32)
    >>> tensor_B = torch.tensor([[7, 10],
                                [8, 11],
                                [9, 12]], dtype=torch.float32)
    >>> tensor_A @ tensor_B
    RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)

-> error vì không đúng chiều, ta có thể xem lại cách nhân ma trận bên đại số tuyến tính ta sẽ giải quyết vấn đề này
- Để giải quyết vấn đề trên ta có thê thực hiện các phương thức sau:
    + torch.transpose(tensor, dim0, dim1): khi mà abs(dim0 - dim1): chẵn thì giữ nguyên, lẻ thì đổi chiều
    + tensor.T
    >>> tenror_A.T @ tensor_B
    tensor([[ 76., 103.],
            [100., 136.]]

_______________________________________________________________________
12. Lớp torch.nn.Linear()
- Tiếp đến tối giới thiệu cho bạn lớp: torch.nn.linear(): còn được gọi là lớp kết nối đầy đủ (full connect layer)
    + để tạo đầu ra trong DL thì ta nhân ma trận trọng số W với đầu vào x và cộng với hệ số bias để ta thu được Y
    => Y = X @ W.T + B
    Tại đây: X: đầu vào của 1 lớp, (trong học sâu, để tạo nên model thì ta chồng lên nhau các lớp như lirnear ở trên)
             Y: Kết quả đầu ra sau khi nhân W với X + B
             W: là trọng số trong DL, ban đầu nó được tạo ngẫu nhiên và sau đó nó học tập từ dữ liệu vào xây dựng lại
             trọng số đó (Các bạn chú ý .T là transpose chuyển vị)
             B: trọng số bias được dùng để bù đặp lại thiếu sót của trọng số hay của bất kì model nào

- Ngay từ cái tên bạn đã hiểu là tuyến tính, bạn có hình dung nó rất quen với thứ gì đó mà chúng ta đã học trong 12 năm
học toán không, và đó là đường thăng y = ax + b mà mình đã được học rồi đó

- ở đây ta học được Method khác:
    + torch.manual_seed(number): khi sử dụng cái này thì bạn hiểu nó được dùng như trong random.seed() là nó sẽ tái tạo
    lại cái random ban đầu, khác với random là cái method này phục vụ cho các lớp ngẫu nhiên như linear()
    + torch.nn.Linear(): nó tự tạo ngẫu nhiên trọng số weight ban đầu, và để không bị thay đổi nhiều là ta dùng method
    trên

- Ta thử làm gì đó với hàm:
EX:
    >>> torch.manual_seed(42)
    >>> linear = torch.nn.Linear(in_features=2, out_features=6)
    >>> print(linear.weight)
    >>> x = tensor_A
    >>> output = linear(x)
    >>> print(f"Input shape: {x.shape}\n")
    >>> print(f"Output:\n{output}\n\nOutput shape: {output.shape}")
    # output = (3, 2) @ (2, 6) => (3, 6)
    Parameter containing:
    tensor([[ 0.5406,  0.5869],
            [-0.1657,  0.6496],
            [-0.1549,  0.1427],
            [-0.3443,  0.4153],
            [ 0.6233, -0.5188],
            [ 0.6146,  0.1323]], requires_grad=True)
    Input shape: torch.Size([3, 2])

    Output:
    tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],
            [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],
            [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],
           grad_fn=<AddmmBackward0>)

    Output shape: torch.Size([3, 6])

Câu hỏi: Điều gì xảy ra nếu bạn thay đổi in_features từ 2 thành 3 ở trên? Nó có báo lỗi không? Làm cách nào bạn có thể
thay đổi hình dạng của đầu vào (x) để phù hợp với lỗi? Gợi ý: chúng ta phải làm gì với tensor_B ở trên?

* Trong DL thì việc nhân matrix  rất là quan trọng
    + Nếu bạn chưa từng làm việc này trước đây thì phép nhân ma trận ban đầu có thể là một chủ đề khó hiểu.
    + Nhưng sau khi bạn thử nghiệm nó vài lần và thậm chí mở được một vài mạng lưới thần kinh, bạn sẽ nhận thấy nó ở
    khắp mọi nơi.
    + Hãy nhớ rằng, phép nhân ma trận là tất cả những gì bạn cần.
    + Khi bạn bắt đầu tìm hiểu sâu hơn về các lớp mạng thần kinh và xây dựng lớp mạng của riêng mình, bạn sẽ tìm thấy
    các phép nhân ma trận ở khắp mọi nơi.

VÌ THẾ ĐIỀU RẤT QUAN TRỌNG TRONG DL VÀ NN LÀ NHÂN MA TRẬN: NẾU CÁC BẠN CHƯA BIẾT CÁC THÌ HỌC ĐỌC SÁCH ĐẠI SỐ TUYẾN TÍNH
(LINEAR ALGEBRA)

_______________________________________________________________________
13. Tìm min, max, mean, std, .....

- Ở đây ta tìm hiểu thêm Method mới:
Syntax: torch.arange(start, end, step)
    >>> torch.arange(0, 10, 1)
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

- Ngoại trừ mean thì ta cần chú ý là nó chỉ áp dụng với dtype=float
EX:
    >>> print(f"Minimum: {x.min()}")
    >>> print(f"Maximum: {x.max()}")
    Minimum: 0
    Maximum: 9

    # print(f"Mean: {x.mean()}") # this will error
- Để khắc phục lỗi trên ta cần chuyển x sang kiểu dữ liệu float, torch.float(26, 32...)
    -> ta dùng phương thức torch.to(), torch.type()
    >>> print(f"Mean: {x.type(torch.float32).mean()}") # won't work without float datatype
    Mean: 4.5

    >>> print(f"Sum: {x.sum()}")
    Sum: 45

- Thay vì chúng ta truy cập phương thức của nó luôn như trên
    + x = torch.arange(0, 10, 1)
    + x.min(), x.max(), x.to(torch.float16).mean(), x.sum()

- Ta có thể dùng luôn phương thức của đối tượng torch
    + x = torch.arange(0, 10, 1)
    + torch.min(x), torch.max(x), torch.mean(x.type(torch.float64)), torch.sum(x)

EX:
    >>> torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)
    (tensor(9), tensor(0), tensor(4.5000), tensor(45))

_______________________________________________________________________
14. Vị trí min/max

- Với phần 13 ở trên thì ta thấy nó trả về giá trị min, max của tensor
- Và trong phần này thay vì trả về giá trị min, max thì nó trả về vị trí của giá trị min, max đó trong tensor đó

Syntax: tensor.argmax(), tensor.argmin() : index bắt đầu từ 0

    >>> tensor = torch.arange(10)
    >>> print("tensor:", tensor)
    tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

    >>> print("index min:", tensor.argmin().item())
    index min: 0

    >>> print("index max:", tensor.argmax().item())
    index max: 9

_______________________________________________________________________
15. Thay đổi kiểu dữ liệu trong tensor

- Thay đổi kiểu dữ liệu là quan trọng trong tensor vì bạn có thể thấy các lỗi cơ bản:
    + lỗi mà bạn đã thấy ở trên là torch.mean() - chỉ phục vụ với torch.float
    + khi mà bạn cố thức hiện ép kiểu torch.int32 -> torch.int16 -> error

- Vì thế bạn có thể thay đổi dtype cho phù hợp:
Syntax: torch.Tensor.type()
Ex:
    >>> tenInt16 = torch.tensor([1, 2, 3], dtype=torch.int16)
    >>> tenInt16
    tensor([1, 2, 3], dtype=torch.int16)

    >>> tenInt16 = tenInt16.type(torch.int64)
    >>> tenInt16.dtype
    torch.int64

- Bài tập: Cho đến nay, chúng ta đã đề cập đến một số phương pháp tensor khá tốt nhưng còn rất nhiều phương pháp khác
trong torch. [https://pytorch.org/docs/stable/tensors.html] tôi khuyên bạn nên dành 10 phút để cuộn qua và xem xét bất
kỳ phương pháp nào thu hút sự chú ý của bạn. Nhấp vào chúng và sau đó tự viết chúng ra mã để xem điều gì sẽ xảy ra.

_______________________________________________________________________
16. Reshaping, stacking, squeezing, unsqueezing

- Bình thường chúng ta sẽ cần định hình dạng lại dữ liệu để cho phụ hợp với đầu vào của model, có thể là Flatten, ....
- Các Method cơ bản:

    >>> ten = torch.arange(0, 10)
    >>> ten
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

    + torch.reshape(input, shape) hoặc torch.Tensor.reshape()
    >>> ten.reshape(2, 5)
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    >>> torch.reshape(ten, shape=(2, 5))
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])

    + torch.Tensor.view(shape) or torch.view_copy(input, size): trả về dữ liệu tensor trong hình dạng shape và dữ liệu
    của tensor ban đầu
    >>> ten.view(2, 5)
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    >>> torch.view_copy(ten, size=(5, 2)
    tensor([[0, 1],
            [2, 3],
            [4, 5],
            [6, 7],
            [8, 9]])

    + torch.stack(tensor, dim=0) : nối các tensor theo chiều dim và các tensor phải cùng size
        -> dim = 0 thì chồng dần các tensor lên nhau
        -> dim = 1 thì chồng từng phần tử lên rồi chồng lên nhau xem ví dụ
    >>> ten1 = torch.arange(10).reshape(2,5)
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    >>> ten2 = torch.arange(10, 20).reshape(2, 5)
    tensor([[10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]])

    >>> torch.stack((ten1, ten2), dim=0)
    tensor([[[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9]],

            [[10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]]])
    >>> torch.stack((ten1, ten2), dim=1)
    tensor([[[ 0,  1,  2,  3,  4],
            [10, 11, 12, 13, 14]],

            [[ 5,  6,  7,  8,  9],
            [15, 16, 17, 18, 19]]])

    + torch.squeeze(input): xóa đi chiều là 1
    >>> ten
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

    >>> a = ten.reshape(10, 1)
    >>> a
    tensor([[0],
            [1],
            [2],
            [3],
            [4],
            [5],
            [6],
            [7],
            [8],
            [9]])
    >>> a.shape
    torch.Size([10, 1])

    >>> b = a.squeeze()
    >>> b
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    >>> b.shape
    torch.Size([10])

    + torch.unsqueeze(input): thêm chiều 1 vào shape
    >>> b.unsqueeze(dim=0)
    tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])

    >>> b.unsqueeze(dim=1)
    tensor([[0],
            [1],
            [2],
            [3],
            [4],
            [5],
            [6],
            [7],
            [8],
            [9]])

    + torch.permute(input, dims) : trả về chế độ xem hoán vị số chiều trong tensor
        -> ví dụ chiều là (224: index0, 250: index1, 3: index2) và khi ta permute nó kiểu (2, 1, 0)
            => chiều sẽ thay đổi theo chỉ sô (3, 250, 224)
        -> kiểu reshape: nhưng đây là đổi chiều thay vì hình dạng
    # a dạng (10: index0, 1: index1)
    >>> a.permute(0, 1) # vẫn là shape(10, 1)
    # torch.Size([10, 1])
    tensor([[0],
            [1],
            [2],
            [3],
            [4],
            [5],
            [6],
            [7],
            [8],
            [9]])
    >>> a.permute(1, 0) # shape thay đổi vị trí 1 là 1, 0 là 10 -> shape(10, 1) => shape(1, 10)
    # torch.Size([1, 10])
    tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])

_______________________________________________________________________
17. Chỉ số, lấy dữ liệu tử tensor

- Làm việc với dữ liệu bạn cần phải biết truy xuất dữ liệu để phân tích nó ra
- Nếu bạn đã từng làm với object list or Numpy array thì tensor cũng tương tự như vậy:
- Điều đặc biệt ở trong tensor là giống numpy về cách thức là slicing
EX:
    >>> import torch
    >>> x = torch.arange(1, 10).reshape(1, 3, 3)
    >>> x, x.shape
    (tensor([[[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]]]),
     torch.Size([1, 3, 3]))

    >>> x[0], x[0][0], x[0][0][0]
    (tensor([[1, 2, 3],
             [4, 5, 6],
             [7, 8, 9]]),
     tensor([1, 2, 3]),
     tensor(1))

    >>> x[:, 0], x[:, 1 : 3, :2] : slicing như trong numpy.array()
    (tensor([[1, 2, 3]]),
     tensor([[[4, 5],
              [7, 8]]]))

_______________________________________________________________________
18. Pytorch Tensor && Numpy Array

- Khi bạn quen thao tác với numpy hơn thì ta có Method chuyển từ tensor -> numpy

Syntax: torch.Tensor.numpy()
        torch.from_numpy(ndarray)

EX:
    >>> tensor = torch.arange(10)
    >>> tensor
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

    >>> array_numpy = tensor.numpy()
    >>> array_numpy, tensor
    (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))

    >>> import numpy as np
    >>> ndarray = np.arange(10)
    >>> ndarray,
    (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),)

    >>> tensor = torch.from_numpy(ndarray)
    >>> ndarray, tensor
    (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))

- Chú ý rằng điều này không giống trong list gán với nhau:
    + list = list => nó trỏ vào cùng 1 vị trí nên nó thay đổi 1 chỗ thì cái còn lại cũng thay đổi
    + Cái ở trên giứa numpy và tensor thì đã được xử lí và không xảy ra lỗi trên
    + Không trỏ cùng 1 chỗ

_______________________________________________________________________
19. Cố gắng tái sản xuất ( lấy ngẫu nhiên ra khỏi ngẫu nhiên )

- Các hệ số random đôi khi rất là tốt khi bạn khởi tạo, nhưng đổi lúc bạn muốn nó bớt ngẫu nhiên đi ta dùng phương thức
Syntax:
    torch.manual_seed(seed=)

Ex:
    >>> torch.manual_seed(42)
    >>> tensor42 = torch.rand((2, 3))
    >>> tensor42.dtype, tensor42
    (torch.float32,
    tensor([[0.8823, 0.9150, 0.3829],
           [0.9593, 0.3904, 0.6009]]))

_______________________________________________________________________
20. Chạy tensor trên GPU ( thức hiện tính toán nhanh hơn )
* Mặc định trong máy chạy là cpu
* Ta muốn nó chạy trên Gpu(cuda) bới nó có nhiểu thứ có lợi

a, Nhận GPU
- Cài đặt:
    * Google Colab:
        Độ khó: dễ
        Pros: Sử dụng miễn phí, gần như không cần thiết lập, có thể chia sẻ công việc với người khác dễ dàng như một liên kết
        Cons: Không lưu dữ liệu đầu ra của bạn, tính toán hạn chế, có thể hết thời gian chờ
        Cài đặt: https://colab.research.google.com/notebooks/gpu.ipynb

    * Máy bạn:
        Độ khó: trung bình
        Pros: Chạy mọi thứ cục bộ trên máy của bạn
        Cons: GPU không miễn phí, yêu cầu chi phí trả trước
        Cài đặt: https://pytorch.org/get-started/locally/

    * Cloud (AWS, GCP, Azure):
        Độ khó: Khó
        Pros: Chi phí trả trước nhỏ, quyền truy cập vào điện toán gần như vô hạn
        Cons: Có thể tốn kém nếu chạy liên tục, mất một chút thời gian để thiết lập đúng
        Cài đặt: https://pytorch.org/get-started/cloud-partners/

- Điều bạn nên quan tâm trong phần này nữa là khi ta training model của chúng ta mà ta sử dụng chính máy chúng ta nó có
thể gây ra nhiều bất lợi cho máy trong thời gian dài

- Vì thế mà chúng ta tìm cách có thể training model một cách tốt nhất
- Ta có thể sử dụng Cuda có trong Nvidia GPU (CuDa), để kiểm tra quyền truy cập ta có thể chạy trên jupyter bằng dòng lệnh:
Command: !nvidia-smi

- Sẽ in ra khi có quyền truy cập:

    on Sep 11 19:54:47 2023
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
    | N/A   49C    P8     5W /  50W |    310MiB /  4096MiB |     23%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+

    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |    0   N/A  N/A      2691      G   /usr/lib/xorg/Xorg                149MiB |
    |    0   N/A  N/A      3060      G   /usr/bin/gnome-shell               29MiB |
    |    0   N/A  N/A      3649      G   ...RendererForSitePerProcess       35MiB |
    |    0   N/A  N/A      9148      G   ...462053733738362438,262144       82MiB |
    |    0   N/A  N/A     10427      G   ...RendererForSitePerProcess        9MiB |
    +-----------------------------------------------------------------------------+

- Khi không có quyển truy cập:
        NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA
    driver is installed and running.

b, Sử dụng pytorch chạy trên GPU(cuda)

- Khi bạn đã có sẵn GPU để truy cập, bước tiếp theo là sử dụng PyTorch để lưu trữ dữ liệu (tensor) và tính toán dữ liệu
(thực hiện các thao tác trên tensor)
- Để làm điều đó bạn có thể sử dụng torch.cuda gói

- Bắt tay vào việc:
    + Ban đầu bạn kiểm tra pytorch có quyền truy cập Gpu sử dụng:
        Syntax:
            >>> torch.cuda.is_available()
            True
        -> Hiện True thì tiếp tục còn hiện, False thì quay lại bước a

    + Khi mà trong máy ta cài đặt các cái chạy rồi thì ta có thể chạy cuda, nhưng khi mà ta chuyển cho khách hàng hay là
    ai đó chạy thì ta nên có câu lệnh device sau (bởi cpu là default)
        Syntax:
            >>> device = "cuda" if torch.cuda.is_available() else "cpu"
            >>> device
            'cuda'
        Cách khác xem ở link: https://pytorch.org/docs/master/notes/cuda.html#device-agnostic-code

    + Nếu bạn muốn nhanh thì dùng Gpu, nhanh hơn nữa thì nhiều Gpu, ta kiểm tra số thiết bị Gpu trên máy
        Syntax:
            >>> torch.cuda.device_count()
            1

    => Biết số lượng cụ thể cpu hay gpu mà pytorch có thể truy cập rất hưu ích khi chạy 1 quy trình cho bạn

c, Thêm tensor (hay model) chạy trên GPU

- Ta có Syntax để có thể thay đổi device default "cpu" bằng cách

Syntax: ta thay thuộc tính bằng device="cuda"

EX:
    >>> tensor = torch.tensor([1, 2, 3])
    >>> tensor, tensor.device
    (tensor([1, 2, 3]), device(type='cpu'))

    >>> tensor_gpu = tensor.to(dtype=torch.float32, device=device)
    >>> tensor_gpu, tensor_gpu.device
    (tensor([1., 2., 3.], device='cuda:0'), device(type='cuda', index=0))
    # Sau khi nó chạy xong nghĩa là: tensor đã chạy trên Cuda thứ 0 có thể nhiều gpu

- Khi bạn ở Gpu ta không thể chuyển tensor sang numpy được
EX:
    >>> tensor_gpu.numpy()
    TypeError: can't convert cuda:0 device type tensor to numpy.
    Use Tensor.cpu() to copy the tensor to host memory first.

d, Bạn muốn chuyển lại Cpu khi không dùng Gpu

- Ta có cách chuyển sau:
Syntax:
    >>> tensor_cpu = tensor_gpu.cpu()
    >>> tensor_cpu, tensor_cpu.device
    (tensor([1., 2., 3.]), device(type='cpu'))

    # Khi về cpu ta có thể chuyển sang numpy
    >>> tensor_cpu.numpy(),
    (array([1., 2., 3.], dtype=float32),)
_______________________________________________________________________
21. Bài Tập

* link: https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/00_pytorch_fundamentals_exercises.ipynb
* link: https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/00_pytorch_fundamentals_exercise_solutions.ipynb

EX1: Đọc tài liệu - Một phần quan trọng của deep learning (và học viết mã nói chung) là làm quen với tài liệu của một
framework nhất định mà bạn đang sử dụng. Chúng tôi sẽ sử dụng tài liệu PyTorch rất nhiều trong suốt phần còn lại của
khóa học này. Vì vậy tôi khuyên bạn nên dành 10-phút đọc phần sau (cũng không sao nếu bây giờ bạn chưa hiểu được một số
thứ, trọng tâm vẫn chưa phải là sự hiểu biết đầy đủ, đó là nhận thức).
    * Xem tài liệu về torch.Tensor: https://pytorch.org/docs/stable/tensors.html#torch-tensor
                      torch.cuda: https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics

EX2: Tạo tensor random shape (5, 5), (1, 3)
EX3: Thực hiện nhân hai ma trận
EX4: Tạo các seed cho các bài EX2, EX3
EX5: Khi ta có torch.manual_seed() trong cpu nhưng trong Gpu có không?
EX6: Tạo 2 tensor rồi gửi vào Gpu (cần quyền truy cập)
EX7: Tìm giá trị tối đa và tối thiểu của đầu ra của EX2
EX8: Tìm giá trị chỉ số tối đa và tối thiểu của đầu ra
EX9: Tìm cách chuyển 1 tensor từ shape(10, 1, 1, 1) về shape(10)

_______________________________________________________________________
22. Học Thêm

*Link: https://pytorch.org/tutorials/beginner/basics/intro.html
*link: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html
*link: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html









