############################# Mục Lục ##############################################################################
1. Cách cài pytorch vào môi trường pip or anaconda
2. Thêm modul Pytorch vào chương trình
3. Tensor
4. Tạo tensor và cơ bản
5. Tensor random
6. Tensor Zeros và Ones
7. Tạo tensor trong range và Tensor tương tự (tensor like)
8. Kiểu dữ liệu torch
9. Lấy thông tin từ Tensor
10. Thao tác cơ bản với tensor - Toán tử với tensor
11. Một số lỗi chung phổ biến trong DL:  lỗi shape
12. Lớp torch.nn.Linear()
13. Tìm min, max, mean, std, .....
14. Vị trí min/max
15. Thay đổi kiểu dữ liệu trong tensor
16. Reshaping, stacking, squeezing, unsqueezing
17. Chỉ số, lấy dữ liệu tử tensor
####################################################################################################################

_______________________________________________________________________
1. Cách cài pytorch vào môi trường pip or anaconda

- Cài bằng pip:

    update pip: pip install --upgrade pip
    install package: pip install torch torchvision torchaudio

- Cài bằng conda:

    update conda: conda update conda
    install package:    
        + conda create --name <env_name> python=3.11
        + conda activate <env_name>
        + conda install pytorch torchvision torchaudio -c pytorch
    update package: conda update --all , conda update namepackage

_______________________________________________________________________
2. Thêm modul Pytorch vào chương trình

- Cách hiểu tương tự như include, import trong c++, java
+ Syntax:
    >>> import torch
+ Kiểm tra version: 
    >>> torch.__version__
    '1.13.1+cu117'

=> cài đặt thành công

_______________________________________________________________________
3. Tensor

- Tạo khối trong ML và DL
- Tạo nên các khối số đại diện cho dữ liệu
- Ta có thể nói về ảnh
    + ta tạo nên ảnh với chiều [3, 600, 620] như trong phần code
    điều đó tương đương [màu, chiều rộng, chiều cao]
    màu có 3 chiều <=> (R, G, B)
- Tensor bạn có thể hiểu như cách dùng trong numpy

_______________________________________________________________________
4. Tạo tensor và cơ bản

- Bài tập đầu tiên của bạn là đọc tài liệu: https://pytorch.org/docs/stable/tensors.html
trong 10'

- Kiểu dữ liệu: có 3 cách dùng:
    + dùng cho máy 
    + CPU : torch.FloatTensor (float32)
    + GPU : torch.cuda.FloatTensor (float32)
- Sytax:  
    # tạo tensor
    >>> a =  torch.tensor([[1, 2, 3]])
    tensor([[1, 2, 3]])
    # điều này có nghĩa là tạo 1 tensor, type là tensor

    # kiểm tra số chiều
    >>> a.ndim
    2

    # lấy số từ tensor
    >>> a[0, 0].item()
    1
    >>> a[0, 0]
    tensor(1)

    # kiểm tra dạng trong tensor
    # Xác định bằng cách: từng ngoặc 1
    >>> a.shape
    torch.Size([1, 3])

Ví Dụ Biểu Diễn Thực Tế:
- Dữ liệu chúng tôi vừa tạo có thể là số liệu bán hàng của một cửa 
hàng bít tết và bơ hạnh nhân .
    >>> TENSOR = torch.tensor([[[1, 2, 3],
                                [3, 6, 9],
                                [2, 4, 5]]])
    # Hàng đầu tiên ngày 
    # hàng 2 là bít tết
    # hàng 3 hạnh nhân

- Các loại tensor: 
    + 1 dim : vector
    + 2 dim : matrix
    + n dim: tensor đa chiều

_______________________________________________________________________
5. Tensor random

- Thay vì các bạn tự chuẩn bị dữ liệu ta có thể random dữ liệu
- Cách này cũng mang lại hiệu quả nhất định cho model

Syntax:
    >>> rd = torch.rand(size=(3, 4))
    >>> rd
    tensor([[0.7241, 0.8427, 0.8053, 0.9273],
            [0.5204, 0.0860, 0.5999, 0.7973],
            [0.8827, 0.3033, 0.6519, 0.1351]])
    # Trong ví dụ trên thì khi gọi rand với size= (3 , 4), ta được ra một matrix
    # với các phần tử (0, 1)
    # Bạn có thể làm nó với ảnh [224, 224, 3] ([height, width, color_channels])

_______________________________________________________________________
6. Tensor Zeros và Ones

- K cần tạo bằng tay ta có thể dùng phương thức có sẵn trong torch
Syntax:
    >>> zr = torch.zeros(size=(3, 4))
    >>> zr
    tensor([[0., 0., 0., 0.],
            [0., 0., 0., 0.],
            [0., 0., 0., 0.]])
    >>> on = torch.ones(size=(3, 4))
    >>> on
    tensor([[1., 1., 1., 1.],
            [1., 1., 1., 1.],
            [1., 1., 1., 1.]])

    # Mặc định kiểu dữ liệu là float32
    >>> on.dtype
    torch.float32

    # Chúng ta có thể  thay đổi nó
    >>> zr = torch.zeros(size=(3, 4), dtype=torch.int32)
    >>> zr
    tensor([[0, 0, 0, 0],
            [0, 0, 0, 0],
            [0, 0, 0, 0]], dtype=torch.int32)

_______________________________________________________________________
7. Tạo tensor trong range và Tensor tương tự (tensor like)

- tensor like method: full_like, zeros_like, ones_like, rand_like, randint_like
Syntax:
    >>> ar = torch.arange(1, 6, 2) # (start, end, step)
    >>> ar
    tensor([1, 3, 5])

    >>> tensor_like_ar = torch.full_like(ar, 1)
    >>> tensor_like_ar
    tensor([1, 1, 1])
    # full_like: nó chỉ giống hình dạng của ar và điền giá trị khác value=1

    >>> ones_ar = torch.ones_like(ar)
    >>> ones_ar
    tensor([1, 1, 1])

_______________________________________________________________________
8. Kiểu dữ liệu torch

- Kiểu dữ liệu: torch.int(8, 16, 32, 64)
                torch.float(8, 16, 32, 64)
                torch.boolean
- Trong CPU: torch.FloatTensor, torch.DoubleTensor
             torch.ShortTensor,torch.IntTensor, torch.LongTensor
             torch.BoolTensor
- Trong GPU tương tự CPU: thay "torch." bằng "torch.cuda."

Ví dụ:
    # Default datatype for tensors is float32
    >>> float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                                        dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed
                                        device=None, # defaults to None, which uses the default tensor type
                                        requires_grad=False) # if True, operations performed on the tensor are recorded 

    >>> float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device
    (torch.Size([3]), torch.float32, device(type='cpu'))
        
    >>> float_16_tensor = torch.tensor([3.0, 6.0, 9.0],
                                       dtype=torch.float16) # torch.half would also work

    >>> float_16_tensor.dtype
    torch.float16

_______________________________________________________________________
9. Lấy thông tin từ Tensor

- 3 thuộc tính phổ biến trong tensor mà bạn thấy trước đó:
    + shape: Dạng của tensor là gì?
    + device: Thiết bị chạy của tensor là gì?
    + dtype: Loại dữ liệu của các phần tử được lưu trong tensor là gì?
- Tạo ra random tensor thì:
    + tensor.rand(size): mỗi phần tử trong khoảng từ 0 -> 1
    EX:
    >>> rd_tensor = torch.rand(size=(3, 4))
    tensor([[0.2159, 0.3589, 0.9351, 0.7204],
            [0.4696, 0.4310, 0.5163, 0.5026],
            [0.2548, 0.7579, 0.4583, 0.5948]])
    >>> rd_tensor.shape
    torch.Size([3, 4])
    >>> rd_tensor.device
    device(type='cpu')
    >>> rd_tensor.dtype
    torch.float32
- Khi ta chạy các chường trình pytorch, lỗi có thể rất liên quan đến 3 thuộc tính ở trên
và để fix lỗi thì các bạn có thể in ra các thuộc tính để sửa lỗi đơn giản hơn
    + Bạn có thể đặt ra các câu hỏi:
    -> tensor của mình hình dạng gì
    -> kiểu dữ liệu của nó là gì: float32, int64 hay bool
    -> nó đang chạy trên thiết bị gì ở đâu
    Mong bạn thấy điều này thú vị

_______________________________________________________________________
10. Thao tác cơ bản với tensor - Toán tử với tensor

- Trong học sâu, dữ liệu có thể là (ảnh, text, video, audio, ......) và các dữ liệu đó được biểu
diễn dưới hình dạng của 1 tensor cho 1 loại dữ liệu ở trên

- Một mô hình được tạo lên là sự thao tác trên các đối tượng tensor đó và các thuật toán đều liên
quan dến tensor đó

- Các tensor đại diện cho các dữ liệu đầu vào vì nó biểu diễn dữ liệu, nó đại diện cho dữ liệu dầu vào
dữ liệu train, test

- Ta có thể coi tensor là các đồ vật thực tế được chuyển thể biểu diễn cho máy tính hiểu đó là Tensor
trong pytorch

- Các thao tác cơ bản:
    + cộng: +
    + trừ: -
    + nhân: *
    + chia: /
    + nhân ma trận: @ (trong numpy)
  Còn các thao tác khác nữa, nhưng đây là các thao tác cơ bản xây dựng lên mạng NN (neural network)

- Như các bạn đã biết mạng nơ ron được cấu tạo từ các ma trận trọng số W, và bạn xếp chồng các matrix
đó lên nhau và bạn đã tạo nên một mạng nơ ron phức tạp từ các matrix đó
    -> tip: bạn có thể hình dung như bạn đang xếp lego vậy :)))

- Các thao tác cơ bản ở trên bạn có thể hiểu cách dùng của nó giống như trong numpy

a, Toán tử cơ bản: + - * /
* Thao tác 1 tensor với 1 số
-> Từng phần tử 1 trong tensor sẽ thao tác toán tử với các số đó
EX:
    >>> tensor1 = torch.rand((1, 3))
    tensor([[0.9866, 0.0876, 0.4429]])

    >>> tensor1 + 1
    tensor([[1.9866, 1.0876, 1.4429]])
    >>> tensor1 - 1
    tensor([[-0.0134, -0.9124, -0.5571]])
    >>> tensor1 * 2
    tensor([[1.9733, 0.1752, 0.8857]])
    >>> tensor1 / 2
    tensor([[0.4933, 0.0438, 0.2214]])

* Thao tác 1 tensor với 1 tensor khác
-> Từng phần tử đôi một thao tác toán tử với nhau
EX:
    >>> tensor1 = torch.rand((1, 3))
    >>> tensor2 = torch.rand((1, 3))
    >>> tensor1, tensor2
    (tensor([[0.8398, 0.8819, 0.3660]]), tensor([[0.5670, 0.9662, 0.9384]]))

    >>> tensor1 + tensor2
    tensor([[1.4069, 1.8481, 1.3044]])
    >>> tensor1 - tensor2
    tensor([[ 0.2728, -0.0842, -0.5724]])
    >>> tensor1 * tensor2
    tensor([[0.4762, 0.8521, 0.3434]])
    >>> tensor1 / tensor2
    tensor([[1.4812, 0.9128, 0.3900]]) # chú ý phần tử 0

* các bạn cũng không nhất thiết dùng các toán tử + - * / mà nó thể dùng các phương thức
khi các bạn quen với java
    + torch.add()
    + torch.sub()
    + torch.multiply()
    + torch.divide()

b, Nhân Matrix Tensor (toán tử @: trong numpy)

- Nó rất quan trọng cho các mô hình học sâu, phần lớn liên quan đến matrix tensor
- Toán tử : @ (giống numpy)
- Cách nhân matrix bạn đã học trong đại số tuyến tính (linear algebra)
    * Mình sẽ nhăc lại chút quy tăc nhân: cần thiếu nhất là shape(a, b) @ shape(b, c)
    * số dòng của matrix trước bằng số cột matrix sau
    * Và thứ tự rất quan trọng trong nhân matrix
EX: shape
    (3, 2) @ (3, 2) won't work
    (2, 3) @ (3, 2) will work
    (3, 2) @ (2, 3) will work
    (2, 3) @ (3, 2) -> (2, 2)
    (3, 2) @ (2, 3) -> (3, 3)

EX:
    >>> tensor = torch.tensor([1, 2, 3])
    >>> tensor.shape
    torch.Size([3])
    >>> tensor * tensor
    tensor([1, 4, 9])
    >>> torch.matmul(tensor, tensor)
    tensor(14)
    >>> tensor @ tensor
    tensor(14)

- Ở đây ta cũng có thể không dùng toán tử @ mà thay vào đó torch.matmul(), torch.mm()
- Ta có thể xây dựng thủ công nhân matrix:
EX:
# Tự xây dựng mulmatrix
    >>> %%time
    >>> value = 0
    >>> for i in range(len(tensor)):
            value += tensor[i] * tensor[i]
    >>> value
    tensor(14)
    CPU times: user 2.37 ms, sys: 0 ns, total: 2.37 ms
    Wall time: 6.18 ms

# Modul sẵn
    >>> %%time
    >>> torch.matmul(tensor, tensor)
    tensor(14)
    CPU times: user 85 µs, sys: 0 ns, total: 85 µs
    Wall time: 86.8 µs

-> Từ 2 ví dụ trên bạn thấy rằng hàm tạo sẵn của torch có tốc độ chạy rất nhanh so với tự tạo
-> Bạn nên áp dụng torch.matmul() để cải thiện tốc độ của mô hình

_______________________________________________________________________
11. Một số lỗi chung phổ biến trong DL:  lỗi shape

- Phần lớn trong DL là phép nhân và thực hiện các toán tử trên ma trận vì thế mà hình dạng của ma trận cần được kiểm
soát nghiêm ngặt và đúng đinh dạng cho phép nhân. Vì thế lỗi phổ biến trong DL là bị sai về hình dạng
- Cần lưu ý khi ta thực hiện toán tử với matrix thì ta cần chú ý đến dạng của tensor mà chúng ta đang thao tác là gì
- Nên ta có thể biết dạng của nó trước khi thực hiện thao tác với nó
    + Ta kiếm tra bằng thuộc tính shape ta đã học ở trên

EX:
    >>> tensor_A = torch.tensor([[1, 2],
                                [3, 4],
                                [5, 6]], dtype=torch.float32)
    >>> tensor_B = torch.tensor([[7, 10],
                                [8, 11],
                                [9, 12]], dtype=torch.float32)
    >>> tensor_A @ tensor_B
    RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)

-> error vì không đúng chiều, ta có thể xem lại cách nhân ma trận bên đại số tuyến tính ta sẽ giải quyết vấn đề này
- Để giải quyết vấn đề trên ta có thê thực hiện các phương thức sau:
    + torch.transpose(tensor, dim0, dim1): khi mà abs(dim0 - dim1): chẵn thì giữ nguyên, lẻ thì đổi chiều
    + tensor.T
    >>> tenror_A.T @ tensor_B
    tensor([[ 76., 103.],
            [100., 136.]]

_______________________________________________________________________
12. Lớp torch.nn.Linear()
- Tiếp đến tối giới thiệu cho bạn lớp: torch.nn.linear(): còn được gọi là lớp kết nối đầy đủ (full connect layer)
    + để tạo đầu ra trong DL thì ta nhân ma trận trọng số W với đầu vào x và cộng với hệ số bias để ta thu được Y
    => Y = X @ W.T + B
    Tại đây: X: đầu vào của 1 lớp, (trong học sâu, để tạo nên model thì ta chồng lên nhau các lớp như lirnear ở trên)
             Y: Kết quả đầu ra sau khi nhân W với X + B
             W: là trọng số trong DL, ban đầu nó được tạo ngẫu nhiên và sau đó nó học tập từ dữ liệu vào xây dựng lại
             trọng số đó (Các bạn chú ý .T là transpose chuyển vị)
             B: trọng số bias được dùng để bù đặp lại thiếu sót của trọng số hay của bất kì model nào

- Ngay từ cái tên bạn đã hiểu là tuyến tính, bạn có hình dung nó rất quen với thứ gì đó mà chúng ta đã học trong 12 năm
học toán không, và đó là đường thăng y = ax + b mà mình đã được học rồi đó

- ở đây ta học được Method khác:
    + torch.manual_seed(number): khi sử dụng cái này thì bạn hiểu nó được dùng như trong random.seed() là nó sẽ tái tạo
    lại cái random ban đầu, khác với random là cái method này phục vụ cho các lớp ngẫu nhiên như linear()
    + torch.nn.Linear(): nó tự tạo ngẫu nhiên trọng số weight ban đầu, và để không bị thay đổi nhiều là ta dùng method
    trên

- Ta thử làm gì đó với hàm:
EX:
    >>> torch.manual_seed(42)
    >>> linear = torch.nn.Linear(in_features=2, out_features=6)
    >>> print(linear.weight)
    >>> x = tensor_A
    >>> output = linear(x)
    >>> print(f"Input shape: {x.shape}\n")
    >>> print(f"Output:\n{output}\n\nOutput shape: {output.shape}")
    # output = (3, 2) @ (2, 6) => (3, 6)
    Parameter containing:
    tensor([[ 0.5406,  0.5869],
            [-0.1657,  0.6496],
            [-0.1549,  0.1427],
            [-0.3443,  0.4153],
            [ 0.6233, -0.5188],
            [ 0.6146,  0.1323]], requires_grad=True)
    Input shape: torch.Size([3, 2])

    Output:
    tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],
            [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],
            [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],
           grad_fn=<AddmmBackward0>)

    Output shape: torch.Size([3, 6])

Câu hỏi: Điều gì xảy ra nếu bạn thay đổi in_features từ 2 thành 3 ở trên? Nó có báo lỗi không? Làm cách nào bạn có thể
thay đổi hình dạng của đầu vào (x) để phù hợp với lỗi? Gợi ý: chúng ta phải làm gì với tensor_B ở trên?

* Trong DL thì việc nhân matrix  rất là quan trọng
    + Nếu bạn chưa từng làm việc này trước đây thì phép nhân ma trận ban đầu có thể là một chủ đề khó hiểu.
    + Nhưng sau khi bạn thử nghiệm nó vài lần và thậm chí mở được một vài mạng lưới thần kinh, bạn sẽ nhận thấy nó ở
    khắp mọi nơi.
    + Hãy nhớ rằng, phép nhân ma trận là tất cả những gì bạn cần.
    + Khi bạn bắt đầu tìm hiểu sâu hơn về các lớp mạng thần kinh và xây dựng lớp mạng của riêng mình, bạn sẽ tìm thấy
    các phép nhân ma trận ở khắp mọi nơi.

VÌ THẾ ĐIỀU RẤT QUAN TRỌNG TRONG DL VÀ NN LÀ NHÂN MA TRẬN: NẾU CÁC BẠN CHƯA BIẾT CÁC THÌ HỌC ĐỌC SÁCH ĐẠI SỐ TUYẾN TÍNH
(LINEAR ALGEBRA)

_______________________________________________________________________
13. Tìm min, max, mean, std, .....

- Ở đây ta tìm hiểu thêm Method mới:
Syntax: torch.arange(start, end, step)
    >>> torch.arange(0, 10, 1)
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

- Ngoại trừ mean thì ta cần chú ý là nó chỉ áp dụng với dtype=float
EX:
    >>> print(f"Minimum: {x.min()}")
    >>> print(f"Maximum: {x.max()}")
    Minimum: 0
    Maximum: 9

    # print(f"Mean: {x.mean()}") # this will error
- Để khắc phục lỗi trên ta cần chuyển x sang kiểu dữ liệu float, torch.float(26, 32...)
    -> ta dùng phương thức torch.to(), torch.type()
    >>> print(f"Mean: {x.type(torch.float32).mean()}") # won't work without float datatype
    Mean: 4.5

    >>> print(f"Sum: {x.sum()}")
    Sum: 45

- Thay vì chúng ta truy cập phương thức của nó luôn như trên
    + x = torch.arange(0, 10, 1)
    + x.min(), x.max(), x.to(torch.float16).mean(), x.sum()

- Ta có thể dùng luôn phương thức của đối tượng torch
    + x = torch.arange(0, 10, 1)
    + torch.min(x), torch.max(x), torch.mean(x.type(torch.float64)), torch.sum(x)

EX:
    >>> torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)
    (tensor(9), tensor(0), tensor(4.5000), tensor(45))

_______________________________________________________________________
14. Vị trí min/max

- Với phần 13 ở trên thì ta thấy nó trả về giá trị min, max của tensor
- Và trong phần này thay vì trả về giá trị min, max thì nó trả về vị trí của giá trị min, max đó trong tensor đó

Syntax: tensor.argmax(), tensor.argmin() : index bắt đầu từ 0

    >>> tensor = torch.arange(10)
    >>> print("tensor:", tensor)
    tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

    >>> print("index min:", tensor.argmin().item())
    index min: 0

    >>> print("index max:", tensor.argmax().item())
    index max: 9

_______________________________________________________________________
15. Thay đổi kiểu dữ liệu trong tensor

- Thay đổi kiểu dữ liệu là quan trọng trong tensor vì bạn có thể thấy các lỗi cơ bản:
    + lỗi mà bạn đã thấy ở trên là torch.mean() - chỉ phục vụ với torch.float
    + khi mà bạn cố thức hiện ép kiểu torch.int32 -> torch.int16 -> error

- Vì thế bạn có thể thay đổi dtype cho phù hợp:
Syntax: torch.Tensor.type()
Ex:
    >>> tenInt16 = torch.tensor([1, 2, 3], dtype=torch.int16)
    >>> tenInt16
    tensor([1, 2, 3], dtype=torch.int16)

    >>> tenInt16 = tenInt16.type(torch.int64)
    >>> tenInt16.dtype
    torch.int64

- Bài tập: Cho đến nay, chúng ta đã đề cập đến một số phương pháp tensor khá tốt nhưng còn rất nhiều phương pháp khác
trong torch. [https://pytorch.org/docs/stable/tensors.html] tôi khuyên bạn nên dành 10 phút để cuộn qua và xem xét bất
kỳ phương pháp nào thu hút sự chú ý của bạn. Nhấp vào chúng và sau đó tự viết chúng ra mã để xem điều gì sẽ xảy ra.

_______________________________________________________________________
16. Reshaping, stacking, squeezing, unsqueezing

- Bình thường chúng ta sẽ cần định hình dạng lại dữ liệu để cho phụ hợp với đầu vào của model, có thể là Flatten, ....
- Các Method cơ bản:

    >>> ten = torch.arange(0, 10)
    >>> ten
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

    + torch.reshape(input, shape) hoặc torch.Tensor.reshape()
    >>> ten.reshape(2, 5)
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    >>> torch.reshape(ten, shape=(2, 5))
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])

    + torch.Tensor.view(shape) or torch.view_copy(input, size): trả về dữ liệu tensor trong hình dạng shape và dữ liệu
    của tensor ban đầu
    >>> ten.view(2, 5)
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    >>> torch.view_copy(ten, size=(5, 2)
    tensor([[0, 1],
            [2, 3],
            [4, 5],
            [6, 7],
            [8, 9]])

    + torch.stack(tensor, dim=0) : nối các tensor theo chiều dim và các tensor phải cùng size
        -> dim = 0 thì chồng dần các tensor lên nhau
        -> dim = 1 thì chồng từng phần tử lên rồi chồng lên nhau xem ví dụ
    >>> ten1 = torch.arange(10).reshape(2,5)
    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    >>> ten2 = torch.arange(10, 20).reshape(2, 5)
    tensor([[10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]])

    >>> torch.stack((ten1, ten2), dim=0)
    tensor([[[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9]],

            [[10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]]])
    >>> torch.stack((ten1, ten2), dim=1)
    tensor([[[ 0,  1,  2,  3,  4],
            [10, 11, 12, 13, 14]],

            [[ 5,  6,  7,  8,  9],
            [15, 16, 17, 18, 19]]])

    + torch.squeeze(input): xóa đi chiều là 1
    >>> ten
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

    >>> a = ten.reshape(10, 1)
    >>> a
    tensor([[0],
            [1],
            [2],
            [3],
            [4],
            [5],
            [6],
            [7],
            [8],
            [9]])
    >>> a.shape
    torch.Size([10, 1])

    >>> b = a.squeeze()
    >>> b
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    >>> b.shape
    torch.Size([10])

    + torch.unsqueeze(input): thêm chiều 1 vào shape
    >>> b.unsqueeze(dim=0)
    tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])

    >>> b.unsqueeze(dim=1)
    tensor([[0],
            [1],
            [2],
            [3],
            [4],
            [5],
            [6],
            [7],
            [8],
            [9]])

    + torch.permute(input, dims) : trả về chế độ xem hoán vị số chiều trong tensor
        -> ví dụ chiều là (224: index0, 250: index1, 3: index2) và khi ta permute nó kiểu (2, 1, 0)
            => chiều sẽ thay đổi theo chỉ sô (3, 250, 224)
        -> kiểu reshape: nhưng đây là đổi chiều thay vì hình dạng
    # a dạng (10: index0, 1: index1)
    >>> a.permute(0, 1) # vẫn là shape(10, 1)
    # torch.Size([10, 1])
    tensor([[0],
            [1],
            [2],
            [3],
            [4],
            [5],
            [6],
            [7],
            [8],
            [9]])
    >>> a.permute(1, 0) # shape thay đổi vị trí 1 là 1, 0 là 10 -> shape(10, 1) => shape(1, 10)
    # torch.Size([1, 10])
    tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])

_______________________________________________________________________
17. Chỉ số, lấy dữ liệu tử tensor